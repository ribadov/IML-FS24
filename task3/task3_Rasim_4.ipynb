{"cells":[{"cell_type":"markdown","id":"f16163d2dd773fbc","metadata":{"collapsed":false,"id":"f16163d2dd773fbc"},"source":["# Task 3\n","This serves as a template which will guide you through the implementation of this task. It is advised to first read the whole template and get a sense of the overall structure of the code before trying to fill in any of the TODO gaps.\n","This is the jupyter notebook version of the template. For the python file version, please refer to the file `template_solution.py`."]},{"cell_type":"markdown","id":"c95f1a3a9db8e3f9","metadata":{"collapsed":false,"id":"c95f1a3a9db8e3f9"},"source":["First, we import necessary libraries:"]},{"cell_type":"code","execution_count":2,"id":"824a840beb8b323e","metadata":{"id":"824a840beb8b323e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714991986498,"user_tz":-120,"elapsed":17429,"user":{"displayName":"R. I.","userId":"10462238401396752465"}},"outputId":"b8fa29e7-1c81-4f9b-af76-c07a19a71604"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import numpy as np\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, TensorDataset\n","import os\n","import torch\n","from torchvision import transforms\n","import torchvision.datasets as datasets\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.models as models\n","from sklearn.model_selection import KFold\n","from torch.optim import Adam\n","# Add any other imports you need here\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"id":"82adb41ca8c23be6","metadata":{"id":"82adb41ca8c23be6","executionInfo":{"status":"ok","timestamp":1714992015502,"user_tz":-120,"elapsed":255,"user":{"displayName":"R. I.","userId":"10462238401396752465"}}},"outputs":[],"source":["# The device is automatically set to GPU if available, otherwise CPU\n","# If you want to force the device to CPU, you can change the line to\n","# device = torch.device(\"cpu\")\n","# When using the GPU, it is important that your model and all data are on the\n","# same device.\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","source":["# Global Variables\n","global_batch_size = 128\n","global_num_workers = 16\n","global_model = models.resnet18(weights='IMAGENET1K_V1')\n","global_embedding_size = 512\n","\n","# Debug\n","verbose = False\n","testing = False"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V5wEiv86fXgt","executionInfo":{"status":"ok","timestamp":1714992037235,"user_tz":-120,"elapsed":715,"user":{"displayName":"R. I.","userId":"10462238401396752465"}},"outputId":"c2a557fb-db2b-4880-ebc7-62c77ed5826a"},"id":"V5wEiv86fXgt","execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 126MB/s]\n"]}]},{"cell_type":"code","execution_count":5,"id":"6b3d5c760c9c963b","metadata":{"id":"6b3d5c760c9c963b","executionInfo":{"status":"ok","timestamp":1714993742052,"user_tz":-120,"elapsed":1450724,"user":{"displayName":"R. I.","userId":"10462238401396752465"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0d61289d-5994-414f-8c65-56c63aa0f831"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","64\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","128\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","192\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","256\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","320\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","384\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","448\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","512\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","576\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","640\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","704\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","768\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","832\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","896\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","960\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","1024\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","1088\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","1152\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","1216\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","1280\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","1344\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","1408\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","1472\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","1536\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","1600\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","1664\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","1728\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","1792\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","1856\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","1920\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","1984\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","2048\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","2112\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","2176\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","2240\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","2304\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","2368\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","2432\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","2496\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","2560\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","2624\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","2688\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","2752\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","2816\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","2880\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","2944\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","3008\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","3072\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","3136\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","3200\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","3264\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","3328\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","3392\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","3456\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","3520\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","3584\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","3648\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","3712\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","3776\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","3840\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","3904\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","3968\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","4032\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","4096\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","4160\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","4224\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","4288\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","4352\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","4416\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","4480\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","4544\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","4608\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","4672\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","4736\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","4800\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","4864\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","4928\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","4992\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","5056\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","5120\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","5184\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","5248\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","5312\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","5376\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","5440\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","5504\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","5568\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","5632\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","5696\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","5760\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","5824\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","5888\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","5952\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","6016\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","6080\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","6144\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","6208\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","6272\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","6336\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","6400\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","6464\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","6528\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","6592\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","6656\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","6720\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","6784\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","6848\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","6912\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","6976\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","7040\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","7104\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","7168\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","7232\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","7296\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","7360\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","7424\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","7488\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","7552\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","7616\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","7680\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","7744\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","7808\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","7872\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","7936\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","8000\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","8064\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","8128\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","8192\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","8256\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","8320\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","8384\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","8448\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","8512\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","8576\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","8640\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","8704\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","8768\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","8832\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","8896\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","8960\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","9024\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","9088\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","9152\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","9216\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","9280\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","9344\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","9408\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","9472\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","9536\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","9600\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","9664\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","9728\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","9792\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","9856\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","9920\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","9984\n","torch.Size([64, 512, 1, 1])\n","torch.Size([64, 512])\n","10048\n","torch.Size([2, 512, 1, 1])\n","torch.Size([2, 512])\n","10050\n","(10050, 512)\n","[[1.25124359 0.16960905 0.81326652 ... 1.20292616 0.48873088 1.34051359]\n"," [0.26689515 1.27148449 1.75024557 ... 0.92002875 2.21292901 1.04344094]\n"," [0.08518744 0.7247566  1.96145177 ... 1.92969513 0.06690172 1.23630273]\n"," ...\n"," [1.63047075 0.42020369 0.14549707 ... 0.34331241 0.05894409 1.84941912]\n"," [0.95634812 1.22039461 1.7003516  ... 1.14213383 0.70173872 0.26413703]\n"," [0.87327212 0.66167676 0.13926761 ... 0.61963892 1.4105444  1.63541472]]\n"]}],"source":["\"\"\"\n","Transform, resize and normalize the images and then use a pretrained model to extract\n","the embeddings.\n","\"\"\"\n","# TODO: define a transform to pre-process the images\n","# The required pre-processing depends on the pre-trained model you choose\n","# below.\n","# See https://pytorch.org/vision/stable/models.html#using-the-pre-trained-models\n","train_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),  # Resize images to (224, 224)\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),  # Convert images to PyTorch tensors\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize images\n","])\n","\n","train_dataset = datasets.ImageFolder(root=\"/content/drive/MyDrive/Colab Notebooks/IML FS24/task3/dataset/\", transform=train_transforms)\n","# Hint: adjust batch_size and num_workers to your PC configuration, so that you don't\n","# run out of memory (VRAM if on GPU, RAM if on CPU)\n","train_loader = DataLoader(dataset=train_dataset,\n","                          batch_size=64,\n","                          shuffle=False,\n","                          pin_memory=True, num_workers=16)\n","\n","# TODO: define a model for extraction of the embeddings (Hint: load a pretrained model,\n","# more info here: https://pytorch.org/vision/stable/models.html)\n","model = global_model\n","model.to(device)\n","embedding_size = global_embedding_size  # Dummy variable, replace with the actual embedding size once you pick your model\n","num_images = len(train_dataset)\n","embeddings = np.zeros((num_images, embedding_size))\n","\n","# pick your model\n","feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n","\n","# TODO: Use the model to extract the embeddings. Hint: remove the last layers of the\n","# model to access the embeddings the model generates.\n","# Loop through the dataset and extract embeddings\n","with torch.no_grad():\n","  start_idx = 0\n","  for inputs, _ in train_loader:\n","    inputs = inputs.to(device)\n","    outputs = feature_extractor(inputs)\n","    print(outputs.shape)\n","    embed = outputs.resize_(outputs.shape[0], outputs.shape[1])\n","    print(embed.shape)\n","    batch_size = outputs.shape[0]\n","    embeddings[start_idx:start_idx+batch_size] = outputs\n","    start_idx += batch_size\n","    print(start_idx)\n","print(embeddings.shape)\n","print(embeddings)\n","np.save('/content/drive/MyDrive/Colab Notebooks/IML FS24/task3/dataset/embeddings.npy', embeddings)"]},{"cell_type":"code","source":["with torch.no_grad():\n","        start_idx = 0\n","        for inputs, _ in train_loader:\n","            inputs = inputs.to(device)\n","            outputs = model(inputs)\n","            batch_size = outputs.shape[0]\n","            embeddings[start_idx:start_idx+batch_size] = outputs.cpu().numpy()\n","            start_idx += batch_size\n","np.save('/content/drive/MyDrive/Colab Notebooks/IML FS24/task3/dataset/embeddings_2.npy', embeddings)"],"metadata":{"id":"bM2mimksGPK9"},"id":"bM2mimksGPK9","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":6,"id":"335d91cc379d4f6b","metadata":{"id":"335d91cc379d4f6b","executionInfo":{"status":"ok","timestamp":1714993986771,"user_tz":-120,"elapsed":735,"user":{"displayName":"R. I.","userId":"10462238401396752465"}}},"outputs":[],"source":["def get_data(file, train=True):\n","    \"\"\"\n","    Load the triplets from the file and generate the features and labels.\n","\n","    input: file: string, the path to the file containing the triplets\n","          train: boolean, whether the data is for training or testing\n","\n","    output: X: numpy array, the features\n","            y: numpy array, the labels\n","    \"\"\"\n","    triplets = []\n","    with open(file) as f:\n","        for line in f:\n","            triplets.append(line)\n","\n","    # generate training data from triplets\n","    train_dataset = datasets.ImageFolder(root=\"/content/drive/MyDrive/Colab Notebooks/IML FS24/task3/dataset/\",\n","                                         transform=None)\n","    filenames = [s[0].split('/')[-1].replace('.jpg', '') for s in train_dataset.samples]\n","    embeddings = np.load('/content/drive/MyDrive/Colab Notebooks/IML FS24/task3/dataset/embeddings.npy')\n","    # TODO: Normalize the embeddings\n","    embeddings /= np.linalg.norm(embeddings, axis=1, keepdims=True)\n","\n","    #file_to_embedding = {filename: emb for filename, emb in zip(filenames, embeddings)}\n","    file_to_embedding = {}\n","    for i in range(len(filenames)):\n","        file_to_embedding[filenames[i]] = embeddings[i]\n","    X = []\n","    y = []\n","    for t in triplets:\n","        emb = [file_to_embedding[a] for a in t.split()]\n","        X.append(np.hstack([emb[0], emb[1], emb[2]]))\n","        y.append(1)\n","        if train:\n","            X.append(np.hstack([emb[0], emb[2], emb[1]]))\n","            y.append(0)\n","    X = np.vstack(X)\n","    y = np.hstack(y)\n","    return X, y"]},{"cell_type":"markdown","id":"abc48f07a1c0c478","metadata":{"collapsed":false,"id":"abc48f07a1c0c478"},"source":["Hint: adjust batch_size and num_workers to your PC configuration, so that you don't run out of memory (VRAM if on GPU, RAM if on CPU)"]},{"cell_type":"code","execution_count":27,"id":"6daf836a4adb0abe","metadata":{"id":"6daf836a4adb0abe","executionInfo":{"status":"ok","timestamp":1714995032668,"user_tz":-120,"elapsed":327,"user":{"displayName":"R. I.","userId":"10462238401396752465"}}},"outputs":[],"source":["def create_loader_from_np(X, y=None, train=True, batch_size=global_batch_size, shuffle=True, num_workers=global_num_workers):\n","    if train:\n","        dataset = TensorDataset(torch.from_numpy(X).type(torch.float),\n","                                torch.from_numpy(y).type(torch.long))\n","    else:\n","        if y is not None:\n","            dataset = TensorDataset(torch.from_numpy(X).type(torch.float),\n","                                    torch.from_numpy(y).type(torch.long))\n","        else:\n","            dataset = TensorDataset(torch.from_numpy(X).type(torch.float))\n","    loader = DataLoader(dataset=dataset,\n","                        batch_size=batch_size,\n","                        shuffle=shuffle,\n","                        pin_memory=True, num_workers=num_workers)\n","    return loader\n"]},{"cell_type":"markdown","id":"e1baa5918f11a049","metadata":{"collapsed":false,"id":"e1baa5918f11a049"},"source":["TODO: define a model. Here, the basic structure is defined, but you need to fill in the details"]},{"cell_type":"code","execution_count":28,"id":"fcd11318eb7b9488","metadata":{"id":"fcd11318eb7b9488","executionInfo":{"status":"ok","timestamp":1714995034651,"user_tz":-120,"elapsed":239,"user":{"displayName":"R. I.","userId":"10462238401396752465"}}},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self, input_size):\n","        super().__init__()\n","        #self.fc1 = nn.Linear(input_size, 1024)\n","        #self.fc2 = nn.Linear(1024, 512)\n","        #self.fc3 = nn.Linear(512, 256)\n","        #self.fc4 = nn.Linear(256, 1)\n","        self.fc1 = nn.Linear(input_size, 512)\n","        self.fc2 = nn.Linear(512, 256)\n","        self.out = nn.Linear(256, 1, bias=True)\n","\n","    def forward(self, x):\n","        #x = F.relu(self.fc1(x))\n","        #x = F.relu(self.fc2(x))\n","        #x = F.relu(self.fc3(x))\n","        #x = torch.sigmoid(self.fc4(x))\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        #x = self.out(x)\n","        x = torch.sigmoid(self.out(x))\n","        return x"]},{"cell_type":"code","execution_count":29,"id":"6e1b0092e0b13f88","metadata":{"id":"6e1b0092e0b13f88","executionInfo":{"status":"ok","timestamp":1714995040076,"user_tz":-120,"elapsed":3832,"user":{"displayName":"R. I.","userId":"10462238401396752465"}}},"outputs":[],"source":["TRAIN_TRIPLETS = '/content/drive/MyDrive/Colab Notebooks/IML FS24/task3/train_triplets.txt'\n","\n","# load the training data\n","X, y = get_data(TRAIN_TRIPLETS)\n","# Create data loaders for the training data\n","train_loader = create_loader_from_np(X, y, train = True, batch_size=64)\n","# delete the loaded training data to save memory, as the data loader copies\n","#del X\n","#del y"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"Zh6MH7pps2fu","executionInfo":{"status":"ok","timestamp":1714995041484,"user_tz":-120,"elapsed":1412,"user":{"displayName":"R. I.","userId":"10462238401396752465"}}},"outputs":[],"source":["TEST_TRIPLETS = '/content/drive/MyDrive/Colab Notebooks/IML FS24/task3/test_triplets.txt'\n","\n","# repeat for testing data\n","X_test, y_test = get_data(TEST_TRIPLETS, train=False)\n","test_loader = create_loader_from_np(X_test, train = False, batch_size=2048, shuffle=False)\n","#del X_test\n","#del y_test"],"id":"Zh6MH7pps2fu"},{"cell_type":"code","source":["def evaluate_model(model, loader):\n","    model.eval()\n","    total_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            loss = F.binary_cross_entropy(outputs.squeeze(), labels.float(), reduction='sum')\n","            total_loss += loss.item()\n","            predicted = (outputs >= 0.5).int()\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    loss = total_loss / total\n","    acc = correct / total\n","    return loss, acc"],"metadata":{"id":"MZmchsHlnCUa","executionInfo":{"status":"ok","timestamp":1714995041484,"user_tz":-120,"elapsed":3,"user":{"displayName":"R. I.","userId":"10462238401396752465"}}},"id":"MZmchsHlnCUa","execution_count":31,"outputs":[]},{"cell_type":"code","execution_count":32,"id":"28634c90281cd699","metadata":{"id":"28634c90281cd699","executionInfo":{"status":"ok","timestamp":1714995043331,"user_tz":-120,"elapsed":177,"user":{"displayName":"R. I.","userId":"10462238401396752465"}}},"outputs":[],"source":["def train_model(train_loader, val_loader):\n","    model = Net(3 * 512)\n","    model.train()\n","    model.to(device)\n","\n","    criterion = nn.BCELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","    n_epochs = 5\n","\n","    for epoch in range(n_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs.squeeze(), labels.float())\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * inputs.size(0)\n","\n","        epoch_loss = running_loss / len(train_loader.dataset)\n","        print(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {epoch_loss:.4f}\")\n","\n","        # Validate the model\n","        val_loss, val_acc = evaluate_model(model, val_loader)\n","        print(f\"Epoch {epoch+1}/{n_epochs}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n","\n","    return model"]},{"cell_type":"code","execution_count":33,"id":"b2ea99b26c348253","metadata":{"id":"b2ea99b26c348253","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714995427639,"user_tz":-120,"elapsed":380475,"user":{"displayName":"R. I.","userId":"10462238401396752465"}},"outputId":"b770ed67-aceb-4e03-bb7e-22a40c67bd99"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5, Train Loss: 0.6658\n","Epoch 1/5, Val Loss: 0.6086, Val Acc: 32.0665\n","Epoch 2/5, Train Loss: 0.5938\n","Epoch 2/5, Val Loss: 0.5886, Val Acc: 32.0852\n","Epoch 3/5, Train Loss: 0.5718\n","Epoch 3/5, Val Loss: 0.5838, Val Acc: 32.0762\n","Epoch 4/5, Train Loss: 0.5581\n","Epoch 4/5, Val Loss: 0.5767, Val Acc: 32.0894\n","Epoch 5/5, Train Loss: 0.5442\n","Epoch 5/5, Val Loss: 0.5649, Val Acc: 32.0947\n","Fold 1: Validation Loss: 0.5649, Validation Accuracy: 32.0947\n","Epoch 1/5, Train Loss: 0.6672\n","Epoch 1/5, Val Loss: 0.6188, Val Acc: 32.0644\n","Epoch 2/5, Train Loss: 0.5940\n","Epoch 2/5, Val Loss: 0.5850, Val Acc: 32.0678\n","Epoch 3/5, Train Loss: 0.5711\n","Epoch 3/5, Val Loss: 0.5831, Val Acc: 32.0889\n","Epoch 4/5, Train Loss: 0.5538\n","Epoch 4/5, Val Loss: 0.5668, Val Acc: 32.0950\n","Epoch 5/5, Train Loss: 0.5409\n","Epoch 5/5, Val Loss: 0.5628, Val Acc: 32.0987\n","Fold 2: Validation Loss: 0.5628, Validation Accuracy: 32.0987\n","Average Validation Loss: 0.5638, Average Validation Accuracy: 32.0967\n"]}],"source":["kf = KFold(n_splits=2, shuffle=True, random_state=42)\n","\n","val_losses = []\n","val_accuracies = []\n","\n","for fold, (train_index, val_index) in enumerate(kf.split(X)):\n","    X_train, X_val = X[train_index], X[val_index]\n","    y_train, y_val = y[train_index], y[val_index]\n","\n","    train_loader = create_loader_from_np(X_train, y_train, train=True, batch_size=64)\n","    val_loader = create_loader_from_np(X_val, y_val, train=False, batch_size=64, shuffle=False)\n","\n","    model = train_model(train_loader, val_loader)\n","\n","    val_loss, val_acc = evaluate_model(model, val_loader)\n","    val_losses.append(val_loss)\n","    val_accuracies.append(val_acc)\n","\n","    print(f\"Fold {fold+1}: Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n","\n","print(f\"Average Validation Loss: {np.mean(val_losses):.4f}, Average Validation Accuracy: {np.mean(val_accuracies):.4f}\")\n","\n","model.eval()\n","predictions = []\n","# Iterate over the test data\n","with torch.no_grad(): # We don't need to compute gradients for testing\n","    for [x_batch] in test_loader:\n","        x_batch= x_batch.to(device)\n","        predicted = model(x_batch)\n","        predicted = predicted.cpu().numpy()\n","        # Rounding the predictions to 0 or 1\n","        predicted[predicted >= 0.5] = 1\n","        predicted[predicted < 0.5] = 0\n","        predictions.append(predicted)\n","    predictions = np.vstack(predictions)\n","np.savetxt(\"/content/drive/MyDrive/Colab Notebooks/IML FS24/task3/results_task3_4.txt\", predictions, fmt='%i')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}