{"cells":[{"cell_type":"markdown","id":"57002395eb6b50f8","metadata":{"collapsed":false,"id":"57002395eb6b50f8"},"source":["# Task 4\n","This serves as a template which will guide you through the implementation of this task. It is advised to first read the whole template and get a sense of the overall structure of the code before trying to fill in any of the TODO gaps.\n","This is the jupyter notebook version of the template. For the python file version, please refer to the file `template_solution.py`."]},{"cell_type":"markdown","id":"c0a46aca0e5d9ef","metadata":{"collapsed":false,"id":"c0a46aca0e5d9ef"},"source":["First, we import necessary libraries:"]},{"cell_type":"code","execution_count":1,"id":"b1c6ff7632991155","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2528,"status":"ok","timestamp":1715780507710,"user":{"displayName":"R. I.","userId":"10462238401396752465"},"user_tz":-120},"id":"b1c6ff7632991155","outputId":"e4f4d306-20ae-433e-dedf-fb5a2f86d62f"},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import DistilBertTokenizer, DistilBertModel\n","from transformers import Trainer, TrainingArguments, AdamW \n","from sklearn.metrics import mean_squared_error as mse\n","from dataset import ReviewDataset"]},{"cell_type":"code","execution_count":2,"id":"d5b2d932","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["False\n"]}],"source":["DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(torch.cuda.is_available())\n","BATCH_SIZE = 32  # TODO: Set the batch size according to both training performance and available memory\n","NUM_EPOCHS = 5 # TODO: Set the number of epochs\n","\n","train_val = pd.read_csv(\"train.csv\")\n","test_val = pd.read_csv(\"test_no_score.csv\")"]},{"cell_type":"markdown","id":"6b5cba49","metadata":{"id":"6b5cba49"},"source":["Depending on your approach, you might need to adapt the structure of this template or parts not marked by TODOs.\n","It is not necessary to completely follow this template. Feel free to add more code and delete any parts that are not required."]},{"cell_type":"code","execution_count":3,"id":"161fdafaedaa5b0","metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1715780510220,"user":{"displayName":"R. I.","userId":"10462238401396752465"},"user_tz":-120},"id":"161fdafaedaa5b0"},"outputs":[{"data":{"text/plain":["'class ReviewDataset(Dataset):\\n    def __init__(self, data, tokenizer, max_len):\\n        self.data = data\\n        self.tokenizer = tokenizer\\n        self.max_len = max_len\\n\\n    def __len__(self):\\n        return len(self.data)\\n    \\n    \\n    def __getitem__(self, idx):\\n        title = str(self.data.loc[idx, \\'title\\'])\\n        sentence = str(self.data.loc[idx, \\'sentence\\'])\\n        score = self.data.loc[idx].get(\\'score\\',0.0)\\n\\n        inputs = self.tokenizer(title, sentence, return_tensors=\"pt\", padding = \"max_length\", truncation = True, max_length = self.max_len)\\n        \\n\\n        input_ids = inputs[\\'input_ids\\'].squeeze()\\n        attention_mask = inputs[\\'attention_mask\\'].squeeze()\\n     \\n        return {\\n            \\'input_ids\\': input_ids,\\n            \\'attention_mask\\': attention_mask,\\n            \\'score\\': torch.tensor(score, dtype=torch.float)\\n        }'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"class ReviewDataset(Dataset):\n","    def __init__(self, data, tokenizer, max_len):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.data)\n","    \n","    \n","    def __getitem__(self, idx):\n","        title = str(self.data.loc[idx, 'title'])\n","        sentence = str(self.data.loc[idx, 'sentence'])\n","        score = self.data.loc[idx].get('score',0.0)\n","\n","        inputs = self.tokenizer(title, sentence, return_tensors=\"pt\", padding = \"max_length\", truncation = True, max_length = self.max_len)\n","        \n","\n","        input_ids = inputs['input_ids'].squeeze()\n","        attention_mask = inputs['attention_mask'].squeeze()\n","     \n","        return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'score': torch.tensor(score, dtype=torch.float)\n","        }\"\"\""]},{"cell_type":"code","execution_count":4,"id":"bdc2b41f","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}],"source":["tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","max_len = 512  # Adjust\n","\n","train_dataset = ReviewDataset(train_val, tokenizer, max_len)\n","test_dataset = ReviewDataset(test_val, tokenizer, max_len)\n","\n","train_loader = DataLoader(dataset=train_dataset,\n","                          batch_size=BATCH_SIZE,\n","                          shuffle=True, num_workers=16, pin_memory=True)\n","test_loader = DataLoader(dataset=test_dataset,\n","                         batch_size=BATCH_SIZE,\n","                         shuffle=False, num_workers=16, pin_memory=True)\n","# Additional code if needed"]},{"cell_type":"code","execution_count":5,"id":"843f38b9dbea00b0","metadata":{"executionInfo":{"elapsed":1304,"status":"ok","timestamp":1715783385322,"user":{"displayName":"R. I.","userId":"10462238401396752465"},"user_tz":-120},"id":"843f38b9dbea00b0"},"outputs":[],"source":["class MyModule(nn.Module):\n","    \n","    def __init__(self, premodel):\n","        super().__init__()\n","        self.premodel = premodel\n","        self.fc1 = nn.Linear(self.premodel.config.hidden_size,1)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.premodel(input_ids=input_ids, attention_mask=attention_mask)\n","        x = self.dropout(outputs.last_hidden_state[:,0])\n","        score = self.fc1(x)\n","\n","        return score.squeeze(1)"]},{"cell_type":"code","execution_count":6,"id":"030dff26","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/homebrew/lib/python3.11/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 1 / 5\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 391/391 [1:27:19<00:00, 13.40s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Training Loss: 6.260075278446802\n","Epoch : 2 / 5\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 391/391 [1:25:49<00:00, 13.17s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Training Loss: 1.8470126925526982\n","Epoch : 3 / 5\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 391/391 [1:26:18<00:00, 13.24s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, Training Loss: 1.34878795546339\n","Epoch : 4 / 5\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 391/391 [1:24:54<00:00, 13.03s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, Training Loss: 1.0578099549426447\n","Epoch : 5 / 5\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 391/391 [1:25:03<00:00, 13.05s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, Training Loss: 0.8563787655147446\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 32/32 [02:56<00:00,  5.52s/it]\n"]}],"source":["if __name__ == '__main__':\n","    premodel = DistilBertModel.from_pretrained('distilbert-base-uncased')\n","\n","    model = MyModule(premodel)\n","    torch.save(model.state_dict(), \"Das_Model_Rasim.pth\")\n","    model.load_state_dict(torch.load(\"Das_Model_Rasim.pth\"))\n","    model.to(DEVICE)\n","\n","    optimizer = AdamW(model.parameters(), lr=5e-6)\n","    criterion = nn.MSELoss()\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n","\n","    for epoch in range(NUM_EPOCHS):\n","        \n","        vloss = 0.0\n","        print('Epoch :', epoch+1, '/',NUM_EPOCHS)\n","        \n","        for batch in tqdm(train_loader, total=len(train_loader)):\n","            model.train()\n","            ids = batch['input_ids']\n","            attention_mask = batch['attention_mask']\n","            score = batch['score']\n","            ids, attention_mask, score = ids.to(DEVICE), attention_mask.to(DEVICE), score.to(DEVICE)\n","            optimizer.zero_grad()\n","            \n","            # Forward pass\n","            train_scores = model(input_ids = ids, attention_mask=attention_mask)\n","            loss = criterion(train_scores, score)\n","            vloss += loss.item()\n","\n","            # Backward pass and optimization\n","            loss.backward()\n","            optimizer.step()\n","\n","        scheduler.step()\n","        print(f\"Epoch {epoch+1}, Training Loss: {vloss / len(train_loader)}\")\n","\n","    model.eval()\n","    vloss = 0.0\n","    predictions = []\n","    actuals = []\n","\n","    with torch.no_grad():\n","        results = []\n","        \n","        for batch in tqdm(test_loader, total=len(test_loader)):\n","\n","            ids = batch['input_ids'].to(DEVICE)\n","            attention_mask = batch['attention_mask'].to(DEVICE)\n","            \n","            outputs = model(ids,attention_mask)\n","            \n","            predictions.extend(outputs.cpu().numpy())\n","        \n","        with open(\"result_task4_1.txt\", \"w\") as f:\n","            for val in predictions:\n","                f.write(f\"{val}\\n\")\n","                "]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":5}
